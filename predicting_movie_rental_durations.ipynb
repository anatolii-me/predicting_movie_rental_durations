{"cells":[{"source":"A DVD rental company requires assistance in predicting rental durations based on specified features. Regression models must be developed to forecast the number of rental days, aiming to achieve a Mean Squared Error (MSE) of 3 or less on a test dataset. Successful model implementation will facilitate inventory planning optimization for the company.\n\nRequirements:\n\n- Develop regression models for DVD rental duration prediction.\n- Ensure models achieve a Mean Squared Error (MSE) of 3 or lower on the test dataset.\n- This task entails constructing predictive models tailored to DVD rental durations, with an emphasis on attaining actionable insights for inventory management. Successful completion will contribute to operational efficiency and decision-making processes for the company.\n\nThe data they provided is in the csv file `rental_info.csv`. It has the following features:\n- `\"rental_date\"`: The date (and time) the customer rents the DVD.\n- `\"return_date\"`: The date (and time) the customer returns the DVD.\n- `\"amount\"`: The amount paid by the customer for renting the DVD.\n- `\"amount_2\"`: The square of `\"amount\"`.\n- `\"rental_rate\"`: The rate at which the DVD is rented for.\n- `\"rental_rate_2\"`: The square of `\"rental_rate\"`.\n- `\"release_year\"`: The year the movie being rented was released.\n- `\"length\"`: Lenght of the movie being rented, in minuites.\n- `\"length_2\"`: The square of `\"length\"`.\n- `\"replacement_cost\"`: The amount it will cost the company to replace the DVD.\n- `\"special_features\"`: Any special features, for example trailers/deleted scenes that the DVD also has.\n- `\"NC-17\"`, `\"PG\"`, `\"PG-13\"`, `\"R\"`: These columns are dummy variables of the rating of the movie. It takes the value 1 if the move is rated as the column name and 0 otherwise. For your convinience, the reference dummy has already been dropped.","metadata":{},"id":"b4ae5707-109f-4cd6-8168-88cac0179d6b","cell_type":"markdown"},{"source":"# Importing required modules\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n# Creating a dataframe\nrental_df = pd.read_csv('rental_info.csv', parse_dates=['rental_date', 'return_date'])\n\n# Examining the dataframe for correct values, missing values and numerical distributions\nprint(rental_df.head())\nprint(rental_df.info())\nprint(rental_df.describe())\nprint(rental_df[['release_year']].head(3))\nprint(rental_df['special_features'].unique())\nprint(rental_df.isna().sum())","metadata":{"executionCancelledAt":null,"executionTime":273,"lastExecutedAt":1710257326388,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Importing required modules\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n# Creating a dataframe\nrental_df = pd.read_csv('rental_info.csv', parse_dates=['rental_date', 'return_date'])\n\n# Examining the dataframe for correct values, missing values and numerical distributions\nprint(rental_df.head())\nprint(rental_df.info())\nprint(rental_df.describe())\nprint(rental_df[['release_year']].head(3))\nprint(rental_df['special_features'].unique())\nprint(rental_df.isna().sum())","outputsMetadata":{"0":{"height":616,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"id":"a7ede566-910a-445c-b11a-68d192ac8506","cell_type":"code","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":"                rental_date               return_date  ...  length_2  rental_rate_2\n0 2005-05-25 02:54:33+00:00 2005-05-28 23:40:33+00:00  ...   15876.0         8.9401\n1 2005-06-15 23:19:16+00:00 2005-06-18 19:24:16+00:00  ...   15876.0         8.9401\n2 2005-07-10 04:27:45+00:00 2005-07-17 10:11:45+00:00  ...   15876.0         8.9401\n3 2005-07-31 12:06:41+00:00 2005-08-02 14:30:41+00:00  ...   15876.0         8.9401\n4 2005-08-19 12:30:04+00:00 2005-08-23 13:35:04+00:00  ...   15876.0         8.9401\n\n[5 rows x 15 columns]\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 15861 entries, 0 to 15860\nData columns (total 15 columns):\n #   Column            Non-Null Count  Dtype              \n---  ------            --------------  -----              \n 0   rental_date       15861 non-null  datetime64[ns, UTC]\n 1   return_date       15861 non-null  datetime64[ns, UTC]\n 2   amount            15861 non-null  float64            \n 3   release_year      15861 non-null  float64            \n 4   rental_rate       15861 non-null  float64            \n 5   length            15861 non-null  float64            \n 6   replacement_cost  15861 non-null  float64            \n 7   special_features  15861 non-null  object             \n 8   NC-17             15861 non-null  int64              \n 9   PG                15861 non-null  int64              \n 10  PG-13             15861 non-null  int64              \n 11  R                 15861 non-null  int64              \n 12  amount_2          15861 non-null  float64            \n 13  length_2          15861 non-null  float64            \n 14  rental_rate_2     15861 non-null  float64            \ndtypes: datetime64[ns, UTC](2), float64(8), int64(4), object(1)\nmemory usage: 1.8+ MB\nNone\n             amount  release_year  ...      length_2  rental_rate_2\ncount  15861.000000  15861.000000  ...  15861.000000   15861.000000\nmean       4.217161   2006.885379  ...  14832.841876      11.389287\nstd        2.360383      2.025027  ...   9393.431996      10.005293\nmin        0.990000   2004.000000  ...   2116.000000       0.980100\n25%        2.990000   2005.000000  ...   6561.000000       0.980100\n50%        3.990000   2007.000000  ...  12996.000000       8.940100\n75%        4.990000   2009.000000  ...  21904.000000      24.900100\nmax       11.990000   2010.000000  ...  34225.000000      24.900100\n\n[8 rows x 12 columns]\n   release_year\n0        2005.0\n1        2005.0\n2        2005.0\n['{Trailers,\"Behind the Scenes\"}' '{Trailers}'\n '{Commentaries,\"Behind the Scenes\"}' '{Trailers,Commentaries}'\n '{\"Deleted Scenes\",\"Behind the Scenes\"}'\n '{Commentaries,\"Deleted Scenes\",\"Behind the Scenes\"}'\n '{Trailers,Commentaries,\"Deleted Scenes\"}' '{\"Behind the Scenes\"}'\n '{Trailers,\"Deleted Scenes\",\"Behind the Scenes\"}'\n '{Commentaries,\"Deleted Scenes\"}' '{Commentaries}'\n '{Trailers,Commentaries,\"Behind the Scenes\"}'\n '{Trailers,\"Deleted Scenes\"}' '{\"Deleted Scenes\"}'\n '{Trailers,Commentaries,\"Deleted Scenes\",\"Behind the Scenes\"}']\nrental_date         0\nreturn_date         0\namount              0\nrelease_year        0\nrental_rate         0\nlength              0\nreplacement_cost    0\nspecial_features    0\nNC-17               0\nPG                  0\nPG-13               0\nR                   0\namount_2            0\nlength_2            0\nrental_rate_2       0\ndtype: int64\n"}]},{"source":"# Assigning the correct format to the release_year column\nrental_df['release_year'] = rental_df['release_year'].astype('int')\n\n# Creating a column for the number of rental days, which will be the dependent variable\nrental_df['rental_length_days'] = rental_df['return_date'] - rental_df['rental_date']\nrental_df['rental_length_days'] = rental_df['rental_length_days'].dt.days\n\n# Verification of previous operations\nprint(rental_df[['rental_date', 'return_date', 'rental_length_days']].head())\n\n# Creating dummy variables for \"Behind the Scenes\" and \"Deleted Scenes\"\nrental_df['behind_the_scenes'] = np.where(rental_df['special_features'].str.contains('\"Behind the Scenes\"'), 1, 0)\nrental_df['deleted_scenes'] = np.where(rental_df['special_features'].str.contains('\"Deleted Scenes\"'), 1, 0)\n\n# Verification of previous operations\nprint(rental_df[['special_features', 'behind_the_scenes', 'deleted_scenes']])\n\n# Deleting the original column from the dataframe\nrental_df = rental_df.drop('special_features', axis=1)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1710257326440,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Assigning the correct format to the release_year column\nrental_df['release_year'] = rental_df['release_year'].astype('int')\n\n# Creating a column for the number of rental days, which will be the dependent variable\nrental_df['rental_length_days'] = rental_df['return_date'] - rental_df['rental_date']\nrental_df['rental_length_days'] = rental_df['rental_length_days'].dt.days\n\n# Verification of previous operations\nprint(rental_df[['rental_date', 'return_date', 'rental_length_days']].head())\n\n# Creating dummy variables for \"Behind the Scenes\" and \"Deleted Scenes\"\nrental_df['behind_the_scenes'] = np.where(rental_df['special_features'].str.contains('\"Behind the Scenes\"'), 1, 0)\nrental_df['deleted_scenes'] = np.where(rental_df['special_features'].str.contains('\"Deleted Scenes\"'), 1, 0)\n\n# Verification of previous operations\nprint(rental_df[['special_features', 'behind_the_scenes', 'deleted_scenes']])\n\n# Deleting the original column from the dataframe\nrental_df = rental_df.drop('special_features', axis=1)","outputsMetadata":{"0":{"height":417,"type":"stream"},"1":{"height":340,"type":"dataFrame"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"8ebf4536-40a4-4baa-b68c-d079e63c08ce","outputs":[{"output_type":"stream","name":"stdout","text":"                rental_date               return_date  rental_length_days\n0 2005-05-25 02:54:33+00:00 2005-05-28 23:40:33+00:00                   3\n1 2005-06-15 23:19:16+00:00 2005-06-18 19:24:16+00:00                   2\n2 2005-07-10 04:27:45+00:00 2005-07-17 10:11:45+00:00                   7\n3 2005-07-31 12:06:41+00:00 2005-08-02 14:30:41+00:00                   2\n4 2005-08-19 12:30:04+00:00 2005-08-23 13:35:04+00:00                   4\n                                      special_features  ...  deleted_scenes\n0                       {Trailers,\"Behind the Scenes\"}  ...               0\n1                       {Trailers,\"Behind the Scenes\"}  ...               0\n2                       {Trailers,\"Behind the Scenes\"}  ...               0\n3                       {Trailers,\"Behind the Scenes\"}  ...               0\n4                       {Trailers,\"Behind the Scenes\"}  ...               0\n...                                                ...  ...             ...\n15856  {Trailers,\"Deleted Scenes\",\"Behind the Scenes\"}  ...               1\n15857  {Trailers,\"Deleted Scenes\",\"Behind the Scenes\"}  ...               1\n15858  {Trailers,\"Deleted Scenes\",\"Behind the Scenes\"}  ...               1\n15859  {Trailers,\"Deleted Scenes\",\"Behind the Scenes\"}  ...               1\n15860  {Trailers,\"Deleted Scenes\",\"Behind the Scenes\"}  ...               1\n\n[15861 rows x 3 columns]\n"}],"execution_count":17},{"source":"# Creating a dataframe with independent variables\nX = rental_df.drop(['rental_length_days', 'rental_date', 'return_date'], axis=1)\n\n# Creating a series with a dependent variable\ny = rental_df['rental_length_days']\n\n# Setting a random seed, for repeatability of results\nrandom_seed = 9\n\n# Splitting the data into training and test data in 80/20 proportion\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1710257326492,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Creating a dataframe with independent variables\nX = rental_df.drop(['rental_length_days', 'rental_date', 'return_date'], axis=1)\n\n# Creating a series with a dependent variable\ny = rental_df['rental_length_days']\n\n# Setting a random seed, for repeatability of results\nrandom_seed = 9\n\n# Splitting the data into training and test data in 80/20 proportion\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)"},"cell_type":"code","id":"136d40af-6610-49dd-9903-829da08ae119","outputs":[],"execution_count":18},{"source":"# Creating a dictinary of the models I will use\nmodels = {\n    'knr': KNeighborsRegressor(n_jobs=-1),\n    'line_reg': LinearRegression(n_jobs=-1),\n    'ridge': Ridge(random_state=random_seed),\n    'lasso': Lasso(random_state=random_seed),\n    'dtr': DecisionTreeRegressor(random_state=random_seed),\n    'rfr': RandomForestRegressor(n_jobs=-1, random_state=random_seed),\n    'gbr': GradientBoostingRegressor(random_state=random_seed)\n}\n\n# Creating a dictinary of the hyperparameters for models\nparams = {\n    'knr': {'n_neighbors': list(np.arange(2, 51))},\n    'line_reg': {}, # LinearRegressor has no hyperparams\n    'ridge': {'alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]},\n    'lasso': {'alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]},\n    'dtr': {'max_depth': list(np.arange(1, 51))},\n    'rfr': {'max_depth': list(np.arange(1, 51)), 'n_estimators': [100, 200, 300, 400, 500, 600]},\n    'gbr': {'n_estimators': [100, 200, 300, 400, 500, 600], 'learning_rate': [0.01, 0.05, 0.1, 0.5]}\n}\n\n# Initialization of variables of best model, best model name and best MSE value\nbest_model = None\nbest_model_name = ''\nbest_mse = float('inf')\n\n# Some regression models perform better with standardized data, some like decision tree don't care, however I decided to standardize them to equalize all models\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Creating a loop of enumerating models and their hyperparameters to get the best model and the best MSE value\nfor name, model in models.items():\n    search = RandomizedSearchCV(model, params[name], n_iter=20, scoring='neg_mean_squared_error', cv=5, random_state=random_seed, n_jobs=-1)\n    search.fit(X_train_scaled, y_train)\n    mse = MSE(y_test, search.predict(X_test_scaled))\n    \n    # Output of the best parameters of each model and its result\n    print(f'for {name} best params is {search.best_params_}, MSE = {mse}')\n    \n    # Finding and assigning the best MSE result, the best model and its name to the corresponding variables\n    if mse < best_mse:\n        best_mse = mse\n        best_model = search.best_estimator_\n        best_model_name = name\n        \n# Output results        \nprint(f'The best model is {best_model} with MSE result {mse.round(2)} which is equal to RMSE of {np.sqrt(mse).round(2)}')","metadata":{"executionCancelledAt":null,"executionTime":426010,"lastExecutedAt":1710257752503,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Creating a dictinary of the models I will use\nmodels = {\n    'knr': KNeighborsRegressor(n_jobs=-1),\n    'line_reg': LinearRegression(n_jobs=-1),\n    'ridge': Ridge(random_state=random_seed),\n    'lasso': Lasso(random_state=random_seed),\n    'dtr': DecisionTreeRegressor(random_state=random_seed),\n    'rfr': RandomForestRegressor(n_jobs=-1, random_state=random_seed),\n    'gbr': GradientBoostingRegressor(random_state=random_seed)\n}\n\n# Creating a dictinary of the hyperparameters for models\nparams = {\n    'knr': {'n_neighbors': list(np.arange(2, 51))},\n    'line_reg': {}, # LinearRegressor has no hyperparams\n    'ridge': {'alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]},\n    'lasso': {'alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]},\n    'dtr': {'max_depth': list(np.arange(1, 51))},\n    'rfr': {'max_depth': list(np.arange(1, 51)), 'n_estimators': [100, 200, 300, 400, 500, 600]},\n    'gbr': {'n_estimators': [100, 200, 300, 400, 500, 600], 'learning_rate': [0.01, 0.05, 0.1, 0.5]}\n}\n\n# Initialization of variables of best model, best model name and best MSE value\nbest_model = None\nbest_model_name = ''\nbest_mse = float('inf')\n\n# Some regression models perform better with standardized data, some like decision tree don't care, however I decided to standardize them to equalize all models\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Creating a loop of enumerating models and their hyperparameters to get the best model and the best MSE value\nfor name, model in models.items():\n    search = RandomizedSearchCV(model, params[name], n_iter=20, scoring='neg_mean_squared_error', cv=5, random_state=random_seed, n_jobs=-1)\n    search.fit(X_train_scaled, y_train)\n    mse = MSE(y_test, search.predict(X_test_scaled))\n    \n    # Output of the best parameters of each model and its result\n    print(f'for {name} best params is {search.best_params_}, MSE = {mse}')\n    \n    # Finding and assigning the best MSE result, the best model and its name to the corresponding variables\n    if mse < best_mse:\n        best_mse = mse\n        best_model = search.best_estimator_\n        best_model_name = name\n        \n# Output results        \nprint(f'The best model is {best_model} with MSE result {mse.round(2)} which is equal to RMSE of {np.sqrt(mse).round(2)}')","outputsMetadata":{"0":{"height":197,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"cdcbea1e-14cd-4cfa-8beb-38bf188d2993","outputs":[{"output_type":"stream","name":"stdout","text":"for knr best params is {'n_neighbors': 4}, MSE = 2.748266624645446\nfor line_reg best params is {}, MSE = 2.9417238646975976\nfor ridge best params is {'alpha': 0.5}, MSE = 2.9417965311524554\nfor lasso best params is {'alpha': 0.001}, MSE = 2.9417134890202408\nfor dtr best params is {'max_depth': 26}, MSE = 2.1620462887289804\nfor rfr best params is {'n_estimators': 100, 'max_depth': 16}, MSE = 2.022496586366581\nfor gbr best params is {'n_estimators': 600, 'learning_rate': 0.5}, MSE = 1.8992597665407769\nThe best model is GradientBoostingRegressor(learning_rate=0.5, n_estimators=600, random_state=9) with MSE result 1.9 which is equal to RMSE of 1.38\n"}],"execution_count":19}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}